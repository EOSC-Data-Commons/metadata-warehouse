services:
    broker:
        image: redis:8-alpine
        restart: unless-stopped
        command: redis-server /usr/local/etc/redis/redis.conf
        volumes:
            - broker-data:/data
            - ./etc/redis.conf:/usr/local/etc/redis/redis.conf
        networks:
            - warehouse-backend
        healthcheck:
            test: [ "CMD", "redis-cli", "ping" ]
            interval: 10s
            timeout: 5s
            retries: 3

    flower:
        image: mher/flower
        #build:
        #dockerfile: ./docker/transform/Dockerfile
        #context: .
        networks:
            - warehouse-backend
        depends_on:
            broker:
                condition: service_healthy
            celery:
                condition: service_healthy
        volumes:
            #- ./data/results:/var/celery/results
            - flower-data:/data # persist flower db
        environment:
            CELERY_BROKER_URL: redis://broker:6379/0
            #CELERY_RESULT_BACKEND: "file:///var/celery/results"
            CELERY_RESULT_BACKEND: redis://broker:6379/0
        command:
          [
              "celery",
              "flower",
              "--address=0.0.0.0",
              "--loglevel=info",
              "--persistent=True",
              "--db=flower_db"
            #"--without-heartbeat"
            #"--port=5555"
          ]

    celery:
        build:
            dockerfile: ./docker/transform/Dockerfile
            context: .
        volumes:
            - ./src:/code/app
        networks:
            - warehouse-backend
        depends_on:
            broker:
                condition: service_healthy
        command:
          [
              "celery",
              "-A",
              "tasks",
              "worker",
            #"-Q",
            #"batches",
              "-E",
              "--pool=solo", # https://stackoverflow.com/questions/56767461/celery-workerlosterror-worker-exited-prematurely-signal-6-sigabrt
              "--loglevel=INFO"
          ]
        #volumes:
        #- ./data/results:/var/celery/results
        environment:
            CELERY_BROKER_URL: redis://broker:6379/0
            #CELERY_RESULT_BACKEND: "file:///var/celery/results"
            CELERY_RESULT_BACKEND: redis://broker:6379/0
            EMBEDDING_MODEL: "${EMBEDDING_MODEL}"
        healthcheck:
            test: celery -A tasks status
            interval: 10s
            timeout: 5s
            retries: 3

    transform:
        build:
            dockerfile: ./docker/transform/Dockerfile
            context: .
        volumes:
            - ./src:/code/app
        command: [ "fastapi", "run", "--host", "0.0.0.0", "transform.py", "--port", "80" ]
        networks:
            - warehouse-backend
        environment:
            CELERY_BROKER_URL: redis://broker:6379/0
            #CELERY_RESULT_BACKEND: "file:///var/celery/results"
            CELERY_RESULT_BACKEND: redis://broker:6379/0
            POSTGRES_USER: "${POSTGRES_ADMIN}"
            POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
            EMBEDDING_MODEL: "${EMBEDDING_MODEL}"
            CELERY_BATCH_SIZE: "${CELERY_BATCH_SIZE}"
        depends_on:
            postgres:
                condition: service_healthy
            opensearch:
                condition: service_healthy
            broker:
                condition: service_healthy

    postgres:
        image: postgres:17-alpine
        environment:
            POSTGRES_USER: "${POSTGRES_ADMIN}"
            POSTGRES_PASSWORD: "${POSTGRES_PASSWORD}"
        restart: unless-stopped
        volumes:
            - postgres-data:/var/lib/postgresql/data
        networks:
            - warehouse-backend
        healthcheck:
            test: [ "CMD-SHELL", "pg_isready" ]
            interval: 10s
            timeout: 5s
            retries: 3


    pgadmin:
        image: dpage/pgadmin4:9.6
        container_name: pgadmin4_container
        restart: unless-stopped
        depends_on:
            postgres:
                condition: service_healthy
        environment:
            PGADMIN_DEFAULT_EMAIL: "${PGADMIN_ADMIN}"
            PGADMIN_DEFAULT_PASSWORD: "${PGADMIN_DEFAULT_PASSWORD}"
        volumes:
            - pgadmin-data:/var/lib/pgadmin
        networks:
            - warehouse-backend

    opensearch:
        restart: unless-stopped
        image: opensearchproject/opensearch:3.2.0
        networks:
            - warehouse-backend
        environment:
            discovery.type: single-node
            bootstrap.memory_lock: "true"
            OPENSEARCH_JAVA_OPTS: -Xms512m -Xmx512m
            cluster.name: opensearch-cluster
            plugins.security.disabled: "true"
            OPENSEARCH_INITIAL_ADMIN_PASSWORD: ${OPENSEARCH_INITIAL_ADMIN_PASSWORD}
        ulimits:
            memlock:
                soft: -1
                hard: -1
            nofile:
                soft: 65536
                hard: 65536
        volumes:
            - opensearch-data:/usr/share/opensearch/data
        healthcheck:
            test: [ "CMD-SHELL", "curl --silent --fail http://localhost:9200/_cluster/health || exit 1" ]

    opensearch-dashboards:
        restart: unless-stopped
        image: opensearchproject/opensearch-dashboards:3.2.0
        environment:
            - OPENSEARCH_HOSTS=["http://opensearch:9200"]
            - DISABLE_SECURITY_DASHBOARDS_PLUGIN=true
        volumes:
            - opensearch-dashboards-data:/usr/share/opensearch-dashboards/data
        depends_on:
            opensearch:
                condition: service_healthy
        networks:
            - warehouse-backend

    #https://github.com/EOSC-Data-Commons/eoscdcpoc/pkgs/container/eoscdcpoc-frontend
    frontend:
        image: ghcr.io/eosc-data-commons/eoscdcpoc-frontend:latest
        # Removes only targeted frontend artifacts before refreshing them from /webapp.
        command: >
          sh -c 'set -eu
          echo "Existing volume contents BEFORE prune (/frontend-volume)";
          if [ -d /frontend-volume ]; then find /frontend-volume -maxdepth 2 -type f -print || true; else echo "(volume empty or not yet created)"; fi;
          echo "Pruning old frontend artifacts (assets/, *.html, *.svg, *.css, *.js)";
          rm -rf /frontend-volume/assets || true;
          find /frontend-volume -maxdepth 1 -type f \( -name "*.html" -o -name "*.svg" -o -name "*.css" -o -name "*.js" \) -exec rm -f {} + || true;
          echo "Copying updated assets directory";
          if [ -d /webapp/assets ]; then cp -a /webapp/assets /frontend-volume/; fi;
          echo "Copying root-level *.html / *.svg / *.css / *.js";
          find /webapp -maxdepth 1 -type f \( -name "*.html" -o -name "*.svg" -o -name "*.css" -o -name "*.js" \) -exec cp -f {} /frontend-volume/ \+;
          echo "Final volume contents AFTER copy";
          find /frontend-volume -maxdepth 2 -type f -print'
        volumes:
            - frontend-data:/frontend-volume

    # https://github.com/EOSC-Data-Commons/data-commons-mcp/pkgs/container/data-commons-mcp
    mcp:
        restart: unless-stopped
        image: ghcr.io/eosc-data-commons/data-commons-mcp:latest
        depends_on:
            opensearch:
                condition: service_healthy
            frontend:
                condition: service_completed_successfully
        volumes:
            - frontend-data:/src/webapp
        env_file: [ keys.env ]
        environment:
            - RUST_LOG=info
            - OPENSEARCH_URL=http://opensearch:9200
        networks:
            - warehouse-backend

volumes:
    postgres-data:
    pgadmin-data:
    opensearch-data:
    opensearch-dashboards-data:
    broker-data:
    flower-data:
    frontend-data:

networks:
    warehouse-backend:
